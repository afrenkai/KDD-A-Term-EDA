{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "corpus = df['question']\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_size = 10\n",
    "window_size = 2\n",
    "\n",
    "contexts = []\n",
    "targets = []\n",
    "for sequence in sequences:\n",
    "    for i in range (window_size, len(sequence) - window_size):\n",
    "        context = sequence[i - window_size: i] + sequence[i + 1: i + window_size +  1]\n",
    "        target = sequence[i]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "X = np.array(contexts)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim= embedding_size))\n",
    "model.add(Lambda(lambda x: tf.reduce_mean(x, axis = 1)))\n",
    "model.add(Dense(units = vocab_size, activation= 'softmax'))\n",
    "model.save_weights('cbow.weights.h5')\n",
    "model.load_weights('cbow.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.get_weights()\n",
    "print(embeddings)\n",
    "# embeddings = np.array(embeddings).reshape(-1, 1)\n",
    "# pca = PCA(n_components= 2)\n",
    "# reduced_embeddings = pca.fit_transform(embeddings)\n",
    "# plt.figure(figsize =(6, 10))\n",
    "# for i, word in enumerate(tokenizer.word_index.keys()):\n",
    "#     x, y = reduced_embeddings[i]\n",
    "#     plt.scatter(x, y)\n",
    "#     plt.annotate(word, xy = (x, y), xytext = (5, 2), textcoords = 'offset points', ha = 'right', va = 'bottom')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
